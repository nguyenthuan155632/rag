"""
RAG Codebase Search App - Main Application
A Flask-based application for semantic search on codebases using RAG (Retrieval-Augmented Generation)
"""
import os
import ollama
from flask import Flask

from config import (
    SECRET_KEY, UPLOAD_FOLDER, MAX_CONTENT_LENGTH,
    USE_GLM, GLM_API_KEY, GLM_MODEL, GLM_MAX_TOKENS, GLM_TEMPERATURE,
    USE_OLLAMA, OLLAMA_MODEL,
    USE_LOCAL_EMBEDDINGS, EMBEDDING_MODEL
)
from routes import init_routes

# Initialize Flask app
app = Flask(__name__)
app.config['SECRET_KEY'] = SECRET_KEY
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH

# Ensure uploads directory exists
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# Initialize routes
init_routes(app)

if __name__ == '__main__':
    print("ğŸš€ Starting RAG Codebase Search App")
    print("=" * 50)

    # Check configuration
    openai_configured = bool(os.getenv("OPENAI_API_KEY"))
    glm_configured = bool(GLM_API_KEY)

    # Check Ollama availability
    ollama_available = False
    if USE_OLLAMA:
        try:
            ollama.list()
            ollama_available = True
        except Exception as e:
            print(f"âš ï¸ Ollama not available: {e}")

    print(f"ğŸ“Š Configuration:")
    print(f"  GLM: {'âœ… Configured' if glm_configured else 'âŒ Not Configured'} ({GLM_MODEL}, max_tokens={GLM_MAX_TOKENS}, temp={GLM_TEMPERATURE})")
    print(f"  Ollama: {'âœ… Available' if ollama_available else 'âŒ Not Available'} (Local LLM)")
    print(f"  Local Embeddings: {'âœ… Enabled' if USE_LOCAL_EMBEDDINGS else 'âŒ Disabled'} ({EMBEDDING_MODEL})")
    print(f"  OpenAI API: {'âœ… Configured' if openai_configured else 'âŒ Missing (Optional fallback)'}")

    # Determine mode
    if USE_GLM and glm_configured:
        if USE_LOCAL_EMBEDDINGS:
            print(f"  ğŸ¤– Mode: Fully Local (GLM + Local Embeddings)")
        else:
            print(f"  ğŸ¤– Mode: GLM LLM + OpenAI Embeddings")
    elif USE_OLLAMA and ollama_available:
        if USE_LOCAL_EMBEDDINGS:
            print(f"  ğŸ¤– Mode: Fully Local (Ollama + Local Embeddings)")
        else:
            print(f"  ğŸ¤– Mode: Local LLM + OpenAI Embeddings")
    elif openai_configured:
        if USE_LOCAL_EMBEDDINGS:
            print(f"  ğŸ¤– Mode: OpenAI LLM + Local Embeddings")
        else:
            print(f"  ğŸ¤– Mode: OpenAI Only (LLM + Embeddings)")
    else:
        print(f"  âš ï¸  Mode: Not Configured - GLM, Ollama or API keys needed!")

    print("\nğŸ“ Setup Instructions:")
    if USE_GLM and not glm_configured:
        print(f"  ğŸ”‘ Get GLM API key: https://z.ai/manage-apikey/apikey-list")
        print(f"  ğŸ”‘ Set GLM_API_KEY='your-key'")
        print(f"  ğŸ”‘ Set USE_GLM=true")
    
    if not ollama_available and USE_OLLAMA:
        print(f"  ğŸ”§ Install Ollama: https://ollama.ai/download")
        print(f"  ğŸ”§ Pull model: ollama pull {OLLAMA_MODEL}")
        print(f"  ğŸ”§ Start Ollama: ollama serve")

    if openai_configured:
        print(f"  âœ… OpenAI API configured (fallback option)")
    else:
        print(f"  ğŸ”‘ Optional: export OPENAI_API_KEY='sk-your-key' (for fallback)")

    
    # Determine if ready to start
    ready = glm_configured or ollama_available or openai_configured

    if ready:
        if USE_GLM and glm_configured:
            print(f"\nğŸ‰ GLM RAG system ready!")
            if USE_LOCAL_EMBEDDINGS:
                print(f"   âœ… GLM LLM: {GLM_MODEL}")
                print(f"   âœ… Local Embeddings: {EMBEDDING_MODEL}")
            else:
                print(f"   âœ… GLM LLM: {GLM_MODEL}")
                print(f"   âœ… OpenAI Embeddings")
        elif USE_OLLAMA and ollama_available:
            print(f"\nğŸ‰ Local RAG system ready!")
            if USE_LOCAL_EMBEDDINGS:
                print(f"   âœ… Ollama LLM: {OLLAMA_MODEL}")
                print(f"   âœ… Local Embeddings: {EMBEDDING_MODEL}")
                print(f"   ğŸŒ Completely offline - no API costs!")
            else:
                print(f"   âœ… Ollama LLM: {OLLAMA_MODEL}")
                print(f"   âœ… OpenAI Embeddings")
                print(f"   ğŸ’¡ Local LLM + cloud embeddings")
        elif openai_configured:
            print(f"\nâœ… Cloud-based RAG system ready!")

        print(f"ğŸŒ Starting server at: http://localhost:5555")
        print(f"ğŸ’¡ Open your browser and upload a repomix codebase file!")
    else:
        print(f"\nâŒ Cannot start - no LLM configured")
        print(f"ğŸ’¡ Please install Ollama or configure API keys")

    print("=" * 50)

    # Only start the app if we have at least one LLM configured
    if ready:
        app.run(debug=True, host='0.0.0.0', port=5555)
    else:
        print(f"\nğŸ›‘ App not started. Please configure an LLM first.")
        print(f"ğŸ“š See README.md for detailed setup instructions.")
