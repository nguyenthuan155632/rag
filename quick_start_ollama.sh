#!/bin/bash

# Cleanup function to kill processes on port 5555
cleanup() {
    echo ""
    echo "ğŸ›‘ Stopping processes on port 5555..."

    # Kill Flask app process
    if [ ! -z "$APP_PID" ]; then
        kill $APP_PID 2>/dev/null
        echo "âœ… Stopped Flask app (PID: $APP_PID)"
    fi

    # Kill any other processes using port 5555
    PIDS=$(lsof -ti:5555 2>/dev/null)
    if [ ! -z "$PIDS" ]; then
        echo "ğŸ”„ Killing additional processes on port 5555..."
        echo "$PIDS" | xargs kill 2>/dev/null
        echo "âœ… Killed processes: $PIDS"
    fi

    exit 0
}

# Set trap to catch Ctrl+C (SIGINT)
trap cleanup SIGINT

echo "ğŸš€ Quick Start Ollama RAG System"
echo "================================="

# Check if Ollama is installed
if command -v ollama &> /dev/null; then
    echo "âœ… Ollama is installed"
else
    echo "âŒ Ollama not found"
    echo "ğŸ“¥ Installing Ollama..."

    # Detect OS and install
    if [[ "$OSTYPE" == "darwin"* ]]; then
        # macOS
        if command -v brew &> /dev/null; then
            brew install ollama
        else
            echo "âŒ Homebrew not found. Please install Ollama manually:"
            echo "   Visit: https://ollama.ai/download"
        fi
    else
        echo "ğŸ’¡ Please install Ollama manually:"
        echo "   Visit: https://ollama.ai/download"
        exit 1
    fi
fi

# Check if Ollama is running
if curl -s http://localhost:11434/api/tags &> /dev/null; then
    echo "âœ… Ollama is running"
else
    echo "ğŸ”„ Starting Ollama..."
    ollama serve &
    sleep 5

    # Check again
    if curl -s http://localhost:11434/api/tags &> /dev/null; then
        echo "âœ… Ollama started successfully"
    else
        echo "âŒ Failed to start Ollama"
        echo "ğŸ’¡ Please start manually: ollama serve"
        exit 1
    fi
fi

# Check if CodeLlama is available
if ollama list | grep -q "codellama"; then
    echo "âœ… CodeLlama model is available"
else
    echo "ğŸ“¥ Pulling CodeLlama model (this may take a few minutes)..."
    ollama pull codellama

    if ollama list | grep -q "codellama"; then
        echo "âœ… CodeLlama model downloaded successfully"
    else
        echo "âŒ Failed to download CodeLlama"
        echo "ğŸ’¡ Try manually: ollama pull codellama"
        exit 1
    fi
fi

# Create virtual environment if it doesn't exist
if [ ! -d "venv" ]; then
    echo "ğŸ“¦ Creating virtual environment..."
    python3 -m venv venv
fi

# Activate virtual environment
echo "ğŸ”„ Activating virtual environment..."
source venv/bin/activate

# Install Python dependencies
echo "ğŸ“¦ Installing Python dependencies..."
pip install -r requirements.txt

if [ $? -eq 0 ]; then
    echo "âœ… Dependencies installed successfully"
else
    echo "âŒ Failed to install dependencies"
    echo "ğŸ’¡ Try: pip install -r requirements.txt"
    exit 1
fi

echo ""
echo "ğŸ‰ Setup complete!"
echo "ğŸŒ Starting the RAG app..."
echo ""
echo "ğŸ“ What's happening:"
echo "   â€¢ Using Ollama CodeLlama for LLM"
echo "   â€¢ Using local sentence transformers for embeddings"
echo "   â€¢ Completely offline - no API costs!"
echo ""
echo "ğŸš€ Opening browser..."

# Start the app
python app.py &
APP_PID=$!

# Wait a moment for the server to start
sleep 3

# Open browser (macOS)
if [[ "$OSTYPE" == "darwin"* ]]; then
    open http://localhost:5555
elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
    xdg-open http://localhost:5555
fi

echo "âœ… App started at: http://localhost:5555"
echo "ğŸ’¡ Upload a repomix file to start searching your codebase!"
echo ""
echo "ğŸ›‘ To stop: Press Ctrl+C (will clean up all processes on port 5555)"

# Wait for the app
wait $APP_PID