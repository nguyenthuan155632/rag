# ğŸš€ Quick Start Guide

## âš ï¸ Getting "Ollama not available" Error?

This error means Ollama isn't running yet. Here's how to fix it:

### ğŸ¯ Option 1: Automated Setup (Easiest)
```bash
./quick_start_ollama.sh
```
This will:
- Install Ollama (if needed)
- Pull CodeLlama model
- Start Ollama server
- Install Python dependencies
- Launch the app

### ğŸ”§ Option 2: Manual Setup

#### Step 1: Install Ollama
```bash
# macOS
brew install ollama

# Or download from: https://ollama.ai/download
```

#### Step 2: Pull Code Model
```bash
ollama pull codellama
```

#### Step 3: Start Ollama
```bash
ollama serve
```

#### Step 4: Install Dependencies
```bash
pip install -r requirements.txt
```

#### Step 5: Start App
```bash
python app.py
```

### ğŸ” Check Your Setup
```bash
python check_setup.py
```

This will tell you exactly what's configured and what you need to install.

## ğŸ“± What You'll See

When everything is working, you'll see:
```
ğŸš€ Starting RAG Codebase Search App
==================================================
ğŸ“Š Configuration:
  Ollama: âœ… Available (Local LLM)
  Local Embeddings: âœ… Enabled (all-MiniLM-L6-v2)
  ğŸ¤– Mode: Fully Local (Ollama + Local Embeddings)

ğŸ‰ Local RAG system ready!
   âœ… Ollama LLM: codellama
   âœ… Local Embeddings: all-MiniLM-L6-v2
   ğŸŒ Completely offline - no API costs!

ğŸŒ Starting server at: http://localhost:5555
```

## ğŸŒ Access the App

Open your browser and go to: **http://localhost:5555**

You'll see a green badge saying: **"ğŸ  Fully Local: Ollama (codellama) + Local (all-MiniLM-L6-v2)"**

## ğŸ“ Upload Your Codebase

1. Generate a repomix of your codebase
2. Upload the file (.json, .txt, or .md)
3. Start asking questions about your code!

## ğŸ’¡ Pro Tips

- **No internet required** once Ollama is installed
- **Zero API costs** - completely free
- **Your code stays private** - never leaves your machine
- **Works offline** - perfect for sensitive projects

## ğŸ†˜ Still Having Issues?

1. **Check Ollama is running**: `ollama list`
2. **Check model is installed**: `ollama list` (should show codellama)
3. **Check dependencies**: `python check_setup.py`
4. **Try the automated script**: `./quick_start_ollama.sh`

That's it! You'll have a fully local RAG system running in minutes! ğŸ‰